<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
    <title>Simbrain Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <link href="../../../Styles.css" rel="stylesheet" type="text/css">
</head>

<body>
    <a href="../../../SimbrainDocs.html">
        <div class="logo">
            <p><span></span></p>
        </div>
    </a>
    <div id="main_docs">
        <div class="navi">
            <p><a href="../../../SimbrainDocs.html">Simbrain</a> &gt; <a href="../../Network.html">Network</a> &gt; <a href="../synapse.html">Synapses</a> &gt; Subtractive Normalization</p>
        </div>
        <h1>Subtractive Normalization</h1>
        <p><a href="hebbian.html#HebbLearning">Hebbian learning</a> rules suffer from the fact that weights tend to achieve maximum or minimum values. Several variants of Hebbian learning have been introduced to address this issue; Subtractive Normalization is one of them.
        </p>
        <p>Subtractive normalization is a form of Hebbian learning where the sum of the weights attaching to a given neuron is kept relatively constant. This is achieved by subtracting the product of the target neuron activation <em>a<sub>t</sub></em> and the average activation of source neurons <em>a<sub>i</sub></em> attaching to <em>a<sub>t</sub></em>:
        </p>
        <blockquote>
            <p><img src="../equations/SubtractiveNormalization.png" height="85" width="241">
            </p>
        </blockquote>
        <p>In order for the effect of keeping the sum of the weights attached to a neuron constant, those weights must all use this rule.
        </p>
        <p>The strength of this synapse is <a href="../neuron.html#clipping">clipped</a> so as to remain between the lower and upper bounds specified for this synapse. Note that clipping the values of this type of synapse could interfere with its intended effect of keeping the sum of weights attaching to a neuron constant. </p>
        <p>Note that although this method constrains the sum of the weights to some fixed number, it allows for some weights to go off to positive infinity while others are going off simultaneously to negative infinity. </p>
        <p>See Peter Dayan and Larry Abbott,<em> Theoretical Neuroscience, </em>Cambridge, MA: MIT Press, p. 290.&nbsp;
        </p>
        <p>Also see K. Miller&nbsp; and&nbsp; D. MacKay, "The Role of Constraints in Hebbian Learning", <span style="font-style: italic;">Neural Computation</span> 6, 120-126 (1994).
        </p>
        <p><span class="heading">Learning Rate</span>
        </p>
        <blockquote>
            <p>The learning rate &#949. </p>
        </blockquote>
    </div>
</body>

</html>